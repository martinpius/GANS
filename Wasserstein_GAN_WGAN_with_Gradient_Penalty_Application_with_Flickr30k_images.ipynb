{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wasserstein GAN- WGAN with Gradient Penalty-Application with Flickr30k-images",
      "provenance": [],
      "authorship_tag": "ABX9TyNE/t+umIW94a+XoyloLLpy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/GANS/blob/main/Wasserstein_GAN_WGAN_with_Gradient_Penalty_Application_with_Flickr30k_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7iBb1vL2R2A",
        "outputId": "e4dc63f5-f5bc-40f2-91d6-510d48cd2fcf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are on CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)} {e}\\n>>>> please correct {type(e)} and re-load\")\n",
        "  COLAB = False\n",
        "def time_fmt(t: float = 123.198)->float:\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\">>>> hrs: {h} min: {m:>02} sec: {s:>05.2f}\"\n",
        "print(f\">>>> testing the time formating function............\\n>>>> time elapsed\\t{time_fmt()}\")\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are on CoLaB with torch version 1.9.0+cu102\n",
            ">>>> testing the time formating function............\n",
            ">>>> time elapsed\t>>>> hrs: 0 min: 02 sec: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBTrdWm_41aM"
      },
      "source": [
        "# In this manuscript we are going to implement the Wasserstein GAN with gradient Penalty.\n",
        "# This is a typically WGAN with minor changes in the discriminator normalization layer. Here\n",
        "# we use layer-normalization (suggested in the paper). During training we do not employ \n",
        "# gradient cliping since its delay training. The penalt factor introduced to the cost function\n",
        "# is bassically a loss accounted by a merged imageges (the generated and the real images) sfter\n",
        "# passing through the discriminator network. The learning rate used is 1e-5 with a lambda value of 10\n",
        "# as hyper-parameter for the gradient penalty."
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RzibSSU6T6U",
        "outputId": "373028d6-2b1d-40a3-ed4a-c5f1608bcd07"
      },
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, datetime, random\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from tensorflow import summary\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWHZfxya8dmZ"
      },
      "source": [
        "# setup the seed value for reproducability and the GPU device to deterministic.\n",
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "vYipvBwi9OOW",
        "outputId": "eaaa5ae0-9074-4c4f-84e7-9732f8385e69"
      },
      "source": [
        "# Setup the directories for tensorboard:\n",
        "current_time = datetime.datetime.now().timestamp()\n",
        "fake_path = \"logs/tensorboard/wgan_gp_fake/\" + str(current_time)\n",
        "real_path = \"logs/tensorboard/wgan_gp_real/\" + str(current_time)\n",
        "fake_writer = summary.create_file_writer(fake_path)\n",
        "real_writer = summary.create_file_writer(real_path)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-20-af34e50a6e77>\", line 5, in <module>\n",
            "    fake_writer = summary.create_file_writer(fake_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py\", line 516, in create_file_writer_v2\n",
            "    metadata={\"logdir\": logdir})\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py\", line 285, in __init__\n",
            "    self._init_op = init_op_fn(self._resource)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_summary_ops.py\", line 146, in create_summary_file_writer\n",
            "    _ops.raise_from_not_ok_status(e, name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\n",
            "    six.raise_from(core._status_to_exception(e.code, message), None)\n",
            "  File \"<string>\", line 3, in raise_from\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: logs; Transport endpoint is not connected [Op:CreateSummaryFileWriter]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 725, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 709, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.7/posixpath.py\", line 383, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDofTTMd-U9f"
      },
      "source": [
        "# The discriminator class:\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, img_channels, d_features):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Conv2d(img_channels, d_features, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self.__dblock__(d_features, 2*d_features, 4, 2, 1),\n",
        "        self.__dblock__(2*d_features, 4*d_features, 4, 2, 1),\n",
        "        self.__dblock__(4*d_features, 8*d_features, 4, 2, 1),\n",
        "        nn.Conv2d(8*d_features, 1, kernel_size = 4, stride = 2, padding = 0)\n",
        "        )\n",
        "  def __dblock__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels,\n",
        "                  out_channels,\n",
        "                  kernel_size,\n",
        "                  stride,\n",
        "                  padding,\n",
        "                  bias = False),\n",
        "          nn.InstanceNorm2d(out_channels, affine = True),\n",
        "          nn.LeakyReLU(0.2))\n",
        "  def forward(self, input_tensor):\n",
        "    return self.discriminator(input_tensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD4V3qrTC41i"
      },
      "source": [
        "# The generator class -> same as in WGAN-grad-clipping\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, z_dim, img_channels, g_features):\n",
        "    super(Generator, self).__init__()\n",
        "    self.generator = nn.Sequential(\n",
        "        self.__gblock__(z_dim, 16*g_features, kernel_size = 4, stride = 2, padding = 0),\n",
        "        self.__gblock__(16*g_features, 8*g_features, 4, 2, 1),\n",
        "        self.__gblock__(8*g_features, 4*g_features, 4, 2, 1),\n",
        "        self.__gblock__(4*g_features, 2*g_features, 4, 2, 1),\n",
        "        nn.ConvTranspose2d(2*g_features, img_channels, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.Tanh())\n",
        "    \n",
        "  def __gblock__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels,\n",
        "                           out_channels,\n",
        "                           kernel_size,\n",
        "                           stride,\n",
        "                           padding,\n",
        "                           bias = False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU())\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return self.generator(input_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_jqIWNvMjgc"
      },
      "source": [
        "# parameter initializer -> we initialize the parameters to random normal dst\n",
        "def __initializer__(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, mean = 0.0, std = 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CtAJwd_NKeP"
      },
      "source": [
        "# testing the model if it yield the desired outputs:\n",
        "def __test__():\n",
        "  img_channels = 3\n",
        "  g_features = 64\n",
        "  d_features = 64\n",
        "  z_dim = 100\n",
        "  batch_size = 128\n",
        "  noise = torch.randn(batch_size,z_dim, 1, 1)\n",
        "  img = torch.randn(batch_size, img_channels, g_features, d_features)\n",
        "  disc = Discriminator(img_channels, d_features)\n",
        "  gen = Generator(z_dim, img_channels, g_features)\n",
        "  print(f\">>>> generator's output shape: {gen(noise).shape}\\n>>>> discriminator's output shape: {disc(img).shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1CUMnonPXM3"
      },
      "source": [
        "__test__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQtwuwWtRY2w"
      },
      "source": [
        "# Hyperparameters and other useful transformation objects\n",
        "learning_rate = 1e-4\n",
        "batch_size = 128\n",
        "d_features = 64\n",
        "g_features = 64\n",
        "img_size = 64\n",
        "lambda_value = 10\n",
        "z_dim = 100\n",
        "EPOCHS = 3\n",
        "img_channels = 3\n",
        "fixed_noise = torch.randn(batch_size, z_dim, 1, 1).to(device = device)\n",
        "discriminator = Discriminator(img_channels, d_features).to(device = device)\n",
        "generator = Generator(z_dim, img_channels, g_features).to(device = device)\n",
        "__initializer__(discriminator)\n",
        "__initializer__(generator)\n",
        "transforms = transforms.Compose([\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Resize((img_size, img_size)),\n",
        "                                 transforms.Normalize([0.5 for _ in range(img_channels)],\n",
        "                                                      [0.5 for _ in range(img_channels)])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ax2OlO5PaD5"
      },
      "source": [
        "# Loading and pre-processing images from the directory <Google drive>\n",
        "os.chdir(\"/content/drive/MyDrive/WGAN-FLICKR30K\") # save all checkpoints here\n",
        "class MyLoader(Dataset):\n",
        "  def __init__(self, root_dir, csv_dir, transform = None):\n",
        "    super(MyLoader, self).__init__()\n",
        "    self.transform = transform\n",
        "    self.root_dir = root_dir\n",
        "    self.csv_dir = csv_dir\n",
        "    self.dfm = pd.read_csv(csv_dir, error_bad_lines = False)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dfm)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.root_dir, self.dfm.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return image\n",
        "\n",
        "dataset = MyLoader(root_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/images\",\n",
        "                   csv_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/captions.txt\",\n",
        "                   transform = transforms)\n",
        "loader = DataLoader(dataset = dataset, shuffle = True, batch_size = batch_size)\n",
        "x_train_image_batch = next(iter(loader))\n",
        "print(f\">>>> the shape of pre-processed image is: {x_train_image_batch.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bULWs6W3VFBb"
      },
      "source": [
        "# Get the optimizers for the generator and discriminator (Adam with zero-momentum):\n",
        "gen_optim = optim.Adam(params = generator.parameters(), lr = learning_rate, betas = (0, 0.999))\n",
        "disc_optim = optim.Adam(params = discriminator.parameters(), lr = learning_rate, betas = (0, 0.99))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mh_Q4ARJV_Dx"
      },
      "source": [
        "# We now define our gradient penalty function:\n",
        "def gradient_penalty(discriminator, real_img, fake_img, device = 'cpu'):\n",
        "  batch_size, C, H, W = real.shape\n",
        "  # epsilon value is a tensor of numbers btn 0/1 to be used to interpolate the imgs\n",
        "  eps = torch.rand(batch_size, 1, 1, 1).repeat(1, C, H, W).to(device = device)\n",
        "  mixed_img = real_img* eps + fake_img * (1 - eps) # interpolate the real and fake image\n",
        "  mixed_preds = discriminator(mixed_img)\n",
        "\n",
        "  #compute the penalty factor (using l2-norm of the gradients for the mixed images)\n",
        "  grads = torch.autograd.grad(\n",
        "      inputs = mixed_img,\n",
        "      outputs = mixed_preds,\n",
        "      grad_outputs = torch.ones_like(mixed_preds),\n",
        "      create_graph = True,\n",
        "      retain_graph = True)[0]\n",
        "      #reshape the gradient tensor by combining all other columns but batch-size\n",
        "  grads = grads.view(grads.shape[0], -1)\n",
        "  grad_norm = grads.norm(2, dim = 1) # compute the norm \"l2\" across the gradients\n",
        "  GP = (torch.mean((grad_norm - 1)**2))\n",
        "  return GP\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr85_gxJevmo"
      },
      "source": [
        "# The training loop of WGGAN with GP:\n",
        "tic = time.time()\n",
        "step = 0\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"\\n>>>> train begins for epoch [{epoch + 1}]\\n>>>> please wait while the model is training..............................\")\n",
        "  for idx, real in enumerate(tqdm(loader)):\n",
        "    real = real.to(device)\n",
        "    noise = torch.randn(batch_size, z_dim, 1, 1).to(device = device)\n",
        "    fake_imgs = generator(noise)\n",
        "\n",
        "    # training the discriminator using GP:\n",
        "    real_preds = discriminator(real).reshape(-1)\n",
        "    fake_preds = discriminator(fake_imgs).reshape(-1)\n",
        "    real_loss = torch.mean(real_preds)\n",
        "    fake_loss = torch.mean(fake_preds)\n",
        "    gp = gradient_penalty(discriminator, real, fake_imgs, device = device)\n",
        "    disc_loss = (-(real_loss - fake_loss) + lambda_value * gp)\n",
        "    discriminator.zero_grad()\n",
        "    disc_loss.backward(retain_graph = True)\n",
        "    disc_optim.step()\n",
        "\n",
        "    # training the generator as in WGAN with clipping (max [log(D(G(z)))])\n",
        "    gen_preds = discriminator(fake_imgs).reshape(-1)\n",
        "    gen_loss = -torch.mean(gen_preds)\n",
        "    generator.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    gen_optim.step()\n",
        "\n",
        "    # printing summary on screen and tensorboard\n",
        "    if idx % 100 == 0:\n",
        "      print(f\"\\n>>>> end of epoch {epoch + 1}, generator loss: {gen_loss:.4f}, discriminator loss: {disc_loss:.4f}\")\n",
        "      fake_img = generator(fixed_noise)\n",
        "      fake_img_grids = torchvision.utils.make_grid(fake_img[:32], normalize = True)\n",
        "      real_img_grids = torchvision.utils.make_grid(real[:32], normalize = True)\n",
        "      with fake_writer.as_default():\n",
        "        summary.scalar(\"generator loss\", gen_loss.cpu().detach().numpy(), step = step)\n",
        "      with real_writer.as_default():\n",
        "        summary.scalar(\"dscriminator loss\", disc_loss.cpu().detach().numpy(), step = step)\n",
        "      step += 1\n",
        "%tensorboard --logdir logs/tensorboard\n",
        "toc = time.time()\n",
        "print(f\"\\n>>>> time elapsed for training WGAN with GP for 10 epochs is {time_fmt(toc - tic)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dO9-c5On43o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}