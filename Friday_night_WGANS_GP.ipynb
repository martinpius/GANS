{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Friday night WGANS-GP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPBkbuxOg9MSjy4m9caR6lA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/GANS/blob/main/Friday_night_WGANS_GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul7yz9sGEPBv",
        "outputId": "882cd211-93b8-4a1b-dc05-aad9889cdfe7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are on CoLaB with torch version: {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)}: {e}\\n>>>> please correct {type(e)} and reload\")\n",
        "  COLAB = False\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "def time_fmt(t: float = 128.98)->float:\n",
        "  h = int(t /(60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"{h} hrs: {m:>02} min: {s:>05.2f} sec\"\n",
        "print(f\">>>> time formating\\tprinting formated time for the demo\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are on CoLaB with torch version: 1.8.1+cu101\n",
            ">>>> time formating\tprinting formated time for the demo\n",
            ">>>> time elapsed\t0 hrs: 02 min: 08.00 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoMHCV-YJSEH"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from albumentations.pytorch import ToTensor\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import PIL\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY177twuK85c"
      },
      "source": [
        "seed = 1234\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i732hkFXLhHJ"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, img_channels, d_features):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.discriminator = nn.Sequential(\n",
        "        nn.Conv2d(img_channels, d_features, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        self.__dblock__(d_features, 2*d_features, 4, 2, 1),\n",
        "        self.__dblock__(2*d_features, 4*d_features, 4, 2, 1),\n",
        "        self.__dblock__(4*d_features, 8*d_features, 4, 2, 1),\n",
        "        nn.Conv2d(8*d_features, 1, kernel_size = 4, stride = 2, padding = 0))\n",
        "    \n",
        "  def __dblock__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding, bias = False\n",
        "        ),\n",
        "        nn.InstanceNorm2d(out_channels, affine = True),\n",
        "        nn.LeakyReLU(0.2))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return self.discriminator(input_tensor)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5h-BNxvNuzs"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, img_channels, z_dim, g_features):\n",
        "    super(Generator, self).__init__()\n",
        "    self.generator = nn.Sequential(\n",
        "        self.__gblock__(z_dim, 16*g_features, kernel_size = 4, stride = 2, padding = 0),\n",
        "        self.__gblock__(16*g_features, 8*g_features, 4, 2, 1),\n",
        "        self.__gblock__(8*g_features, 4*g_features, 4, 2, 1),\n",
        "        self.__gblock__(4*g_features, 2*g_features, 4, 2, 1),\n",
        "        nn.ConvTranspose2d(2*g_features, img_channels, kernel_size = 4, stride = 2, padding = 1),\n",
        "        nn.Tanh())\n",
        "    \n",
        "  def __gblock__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            bias = False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU())\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return self.generator(input_tensor)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFcLY0mdQj3h"
      },
      "source": [
        "def __test__():\n",
        "  img_channels = 3\n",
        "  H,W,batch_size = 64,64,64\n",
        "  z_dim = 100\n",
        "  rand_img = torch.randn(batch_size, img_channels, W,H)#for discriminator\n",
        "  noise = torch.randn(batch_size, z_dim, 1, 1)#for generator\n",
        "  gen = Generator(img_channels, z_dim, 8)\n",
        "  disc = Discriminator(img_channels, 8)\n",
        "  gen_out = gen(noise)# expected shape [batch_size, img_channels, W, H]\n",
        "  disc_out = disc(rand_img) #expected shape [batch_size, 1, 1, 1]\n",
        "  return f\"gen_out_shape: {gen_out.shape}\\tdisc_out_shape: {disc_out.shape}\""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8uLkvVxbRq3d",
        "outputId": "f81a2c54-bd5d-4b9e-8157-f3de87472dc8"
      },
      "source": [
        "__test__()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gen_out_shape: torch.Size([64, 3, 64, 64])\\tdisc_out_shape: torch.Size([64, 1, 1, 1])'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9w7BAYYRtMz"
      },
      "source": [
        "def __initializer__(model):\n",
        "  for m in model.modules():\n",
        "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "      nn.init.normal_(m.weight.data, mean = 0.00, std = 0.02)\n",
        "      "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmBKjILReZ-w",
        "outputId": "b9b92572-8f80-4a53-8abe-43b083cf07c8"
      },
      "source": [
        "batch_size = 32\n",
        "img_size = 64\n",
        "img_channels =1\n",
        "g_features = 64\n",
        "d_features = 64\n",
        "z_dim = 100\n",
        "EPOCHS = 10\n",
        "lambda_gp = 10\n",
        "disc_iter = 5\n",
        "learning_rate = 1e-4\n",
        "fixed_noise = torch.randn(batch_size, z_dim, 1, 1).to(device = device)\n",
        "discriminator = Discriminator(img_channels, d_features).to(device)\n",
        "generator = Generator(img_channels, z_dim, g_features).to(device)\n",
        "__initializer__(generator)\n",
        "__initializer__(discriminator)\n",
        "transforms = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Normalize([0.5 for _ in range(img_channels)],\n",
        "                                [0.5 for _ in range(img_channels)])])\n",
        "dfm = datasets.MNIST(root = \"friday_wgans/\", transform = transforms, download = True)\n",
        "loader = DataLoader(dataset = dfm, batch_size = batch_size, shuffle = True)\n",
        "x_loader, y_loader = next(iter(loader))\n",
        "print(f\">>>> x_loader_shape: {x_loader.shape}\\ty_loader_shape: {y_loader.shape}\")\n",
        "print(f\"\\n\\n>>>> discriminator graph:\\n{discriminator}\\n\\ngenerator graph:\\n{generator}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> x_loader_shape: torch.Size([32, 1, 64, 64])\ty_loader_shape: torch.Size([32])\n",
            "\n",
            "\n",
            ">>>> discriminator graph:\n",
            "Discriminator(\n",
            "  (discriminator): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (5): Conv2d(512, 1, kernel_size=(4, 4), stride=(2, 2))\n",
            "  )\n",
            ")\n",
            "\n",
            "generator graph:\n",
            "Generator(\n",
            "  (generator): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (4): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUkCd8MBkefc"
      },
      "source": [
        "gen_opt = optim.Adam(params = generator.parameters(), lr = learning_rate, betas = (0.00, 0.999))\n",
        "disc_opt = optim.Adam(params = discriminator.parameters(), lr = learning_rate, betas = (0.00, 0.999))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5-V-dl5mGTQ"
      },
      "source": [
        "def __gp__(discriminator, real_img, fake_img, device = device):\n",
        "  batch_size, C, H, W = real_img.shape\n",
        "  e = torch.randn(batch_size, 1, 1, 1).repeat(1, C, H, W).to(device = device)\n",
        "  ip_img = e*real_img + (1-e)*fake_img\n",
        "  ip_scores = discriminator(ip_img)\n",
        "  grads = torch.autograd.grad(\n",
        "      inputs = ip_img,\n",
        "      outputs = ip_scores,\n",
        "      grad_outputs = torch.ones_like(ip_scores),\n",
        "      create_graph = True,\n",
        "      retain_graph = True)[0]\n",
        "  grads = grads.view(grads.shape[0], -1)\n",
        "  grads_norm = grads.norm(2, dim = 1)\n",
        "  gp = torch.mean((grads_norm-1)**2)\n",
        "  return gp\n",
        "    "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX67lrkUmobg",
        "outputId": "8fd11634-e6ea-4b6c-9ebc-1e4377d02f75"
      },
      "source": [
        "real_writer = SummaryWriter(f\"logs/real_images\")\n",
        "fake_writer = SummaryWriter(f\"logs/fake_images\")\n",
        "step = 0\n",
        "global_tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  tic = time.time()\n",
        "  print(f\"\\n>>>> training starts for epoch: {epoch + 1}\\n>>>> please wait while the model is training....................\")\n",
        "  for idx, (real, _) in enumerate(tqdm(loader)):\n",
        "    real = real.to(device = device)\n",
        "    for _ in range(disc_iter):\n",
        "      noise = torch.randn(batch_size, z_dim, 1, 1).to(device = device)\n",
        "      fake_image = generator(noise)\n",
        "      real_disc_out = discriminator(real).reshape(-1)\n",
        "      fake_disc_out = discriminator(fake_image).reshape(-1)\n",
        "      disc_real_loss = torch.mean(real_disc_out)\n",
        "      disc_fake_loss = torch.mean(fake_disc_out)\n",
        "      gp = __gp__(discriminator, real, fake_image)\n",
        "      disc_loss = (-(disc_real_loss - disc_fake_loss) + lambda_gp * gp)\n",
        "      discriminator.zero_grad()\n",
        "      disc_loss.backward(retain_graph = True)\n",
        "      disc_opt.step()\n",
        "      gen_out = discriminator(fake_image).reshape(-1)\n",
        "      gen_loss = -torch.mean(gen_out)\n",
        "      generator.zero_grad()\n",
        "      gen_loss.backward()\n",
        "      gen_opt.step()\n",
        "      toc = time.time()\n",
        "      if idx % 200 == 0:\n",
        "        print(f\"\\n>>>> time elapsed at the end of epoch {epoch + 1} of batch {idx} is {time_fmt(toc - tic)}\")\n",
        "        print(f\">>>> generator loss: {gen_loss:.4f} | generator PPL: {math.exp(gen_loss):7.4f}\")\n",
        "        print(f\">>>> discriminator loss: {disc_loss:.4f} | discriminator PPL: {math.exp(disc_loss):7.4f}\")\n",
        "        with torch.no_grad():\n",
        "          fake_image = generator(fixed_noise)\n",
        "          real_img_grid = torchvision.utils.make_grid(real[:32], normalize = True)\n",
        "          fake_img_grid = torchvision.utils.make_grid(fake_image[:32], normalize = True)\n",
        "          real_writer.add_image(\"real_images\", real_img_grid, global_step = step)\n",
        "          fake_writer.add_image('fake_images', fake_img_grid, global_step = step)\n",
        "        step+=1\n",
        "global_toc = time.time()\n",
        "print(f\"\\n>>>> time elapsed at the end of training: {time_fmt(global_toc - global_tic)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1875 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch: 1\n",
            ">>>> please wait while the model is training....................\n",
            "\n",
            ">>>> time elapsed at the end of epoch 1 of batch 0 is 0 hrs: 00 min: 07.00 sec\n",
            ">>>> generator loss: 0.4419 | generator PPL:  1.5557\n",
            ">>>> discriminator loss: 138.5393 | discriminator PPL: 1468373439783610395984339173235211649821434926801716217118720.0000\n",
            "\n",
            ">>>> time elapsed at the end of epoch 1 of batch 0 is 0 hrs: 00 min: 15.00 sec\n",
            ">>>> generator loss: 1.0993 | generator PPL:  3.0020\n",
            ">>>> discriminator loss: 88.4659 | discriminator PPL: 263180705147594256973721438383216001024.0000\n",
            "\n",
            ">>>> time elapsed at the end of epoch 1 of batch 0 is 0 hrs: 00 min: 24.00 sec\n",
            ">>>> generator loss: 1.7239 | generator PPL:  5.6066\n",
            ">>>> discriminator loss: 72.7807 | discriminator PPL: 40576376625350739227661177978880.0000\n",
            "\n",
            ">>>> time elapsed at the end of epoch 1 of batch 0 is 0 hrs: 00 min: 32.00 sec\n",
            ">>>> generator loss: 2.5063 | generator PPL: 12.2596\n",
            ">>>> discriminator loss: 39.1812 | discriminator PPL: 103800554868448576.0000\n",
            "\n",
            ">>>> time elapsed at the end of epoch 1 of batch 0 is 0 hrs: 00 min: 40.00 sec\n",
            ">>>> generator loss: 3.2958 | generator PPL: 26.9988\n",
            ">>>> discriminator loss: 32.4959 | discriminator PPL: 129660076724604.7188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1875 [01:17<20:47:08, 39.95s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-7iW42v36ft"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}