{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Photo Realistic- Super-Resolution (SR)-GAN, Pytorch implementation from scratch",
      "provenance": [],
      "authorship_tag": "ABX9TyPrKgSr/SAyf5Gd+utbImXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c2999e50aed4816a12e5379c65e844a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fae13debed2f49028e4ba8b5a6b67c88",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_805ba895e43544128ae5535fe54b1170",
              "IPY_MODEL_f28083dbdba14366948b988e45fdd8c8"
            ]
          }
        },
        "fae13debed2f49028e4ba8b5a6b67c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "805ba895e43544128ae5535fe54b1170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_29429b6576454373a71d1f51787794d1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd92707c124d46e18e376d94411d4026"
          }
        },
        "f28083dbdba14366948b988e45fdd8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a7134d25897472ca7986e7e801628e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:08&lt;00:00, 71.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3ac8bd9487b4cc988cf8ed1e8f1da49"
          }
        },
        "29429b6576454373a71d1f51787794d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd92707c124d46e18e376d94411d4026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a7134d25897472ca7986e7e801628e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3ac8bd9487b4cc988cf8ed1e8f1da49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/GANS/blob/main/Photo_Realistic_Super_Resolution_(SR)_GAN%2C_Pytorch_implementation_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qsrk1FnbS6D",
        "outputId": "41e21480-2b31-4a37-f478-4f98963a03fc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import torch\n",
        "  print(f\">>>> You are on CoLaB with torch version {torch.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\">>>> {type(e)} {e}\\n>>>> please correct {type(e)} and reload your drive\")\n",
        "  COLAB = False\n",
        "def time_fmt(t: float = 123.189)->float():\n",
        "  h = int(t / (60 * 60))\n",
        "  m = int(t % (60 * 60) / 60)\n",
        "  s = int(t % 60)\n",
        "  return f\"hrs: {h} min: {m:>02} sec: {s:>05.2f}\"\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(f\">>>> testing the time formating function........\\n>>>> time elapsed\\t{time_fmt()}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            ">>>> You are on CoLaB with torch version 1.9.0+cu102\n",
            ">>>> testing the time formating function........\n",
            ">>>> time elapsed\thrs: 0 min: 02 sec: 03.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnZO0qhKdENX"
      },
      "source": [
        "# in this notbook we are going to implement the SR-GAN to construct highly resolution images\n",
        "# from low resolution images. More detail about the paper can be found here: \n",
        "# paper url (https://arxiv.org/abs/1609.04802)  "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynJNoqYzgyMb",
        "outputId": "efa6d190-2185-4c88-c69d-157f1a1f8449"
      },
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/33/1c459c2c9a4028ec75527eff88bc4e2d256555189f42af4baf4d7bd89233/albumentations-0.4.6.tar.gz (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-cp37-none-any.whl size=65175 sha256=b5cff43b884ba54c5a14df1ee105f8283abfeac89016e52986e474c45741e830\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/f4/89/56d1bee5c421c36c1a951eeb4adcc32fbb82f5344c086efa14\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS20gvnXd5DZ"
      },
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorflow import summary\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import albumentations as B\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time, datetime, random, os\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ukbx0BmeoBf"
      },
      "source": [
        "# setup the seed value for reproducability and gpu to deterministic:\n",
        "seed = 123\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVP7HoFyfCmB"
      },
      "source": [
        "# The SR-GAN consists of two subnetwork as like in the ussual GANS\n",
        "# The generator follow a ResNet architecture where we have 16 residual\n",
        "# blocks. The start is the conv block and after residual blocks we use\n",
        "# the upsample block (x 4) followed by a conv block.\n",
        "# The discriminantor is a convlution network with different kernels to \n",
        "# classify high resolution (original image) from the upscaled (generated)\n",
        "# high resolution images.\n",
        "# The generator use PReLU() activation while the discriminator use LeakyReLU()\n",
        "# The loss function consists of 3 components (For the generator we have component)\n",
        "# related to perceptual loss and content loss (we use vgg19 to achieve this) and the\n",
        "# discriminator employ the ussual GANS loss."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr-Q_ggugr0d"
      },
      "source": [
        "class CNNBLOCK(nn.Module):\n",
        "  ''' \n",
        "  we will use this block to build every conv layer for the generator and\n",
        "  the discriminator.\n",
        "  '''\n",
        "  def __init__(self, \n",
        "               in_channels,\n",
        "               out_channels,\n",
        "               discriminator = False,\n",
        "               use_bn = True,\n",
        "               use_act = True,\n",
        "               **kwargs):\n",
        "    super(CNNBLOCK, self).__init__()\n",
        "    self.use_act = use_act\n",
        "    self.cnn = nn.Conv2d(in_channels, out_channels, **kwargs, bias = not use_bn)\n",
        "    self.bnorm = nn.BatchNorm2d(out_channels) if use_bn else nn.Identity()\n",
        "    self.activation = (nn.LeakyReLU(0.2, inplace = True) \n",
        "    if discriminator\n",
        "    else nn.PReLU(num_parameters = out_channels))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    return (self.activation(self.bnorm(self.cnn(input_tensor))) \n",
        "    if self.use_act else self.bnorm(self.cnn(input_tensor)))\n",
        "\n",
        "\n",
        "\n",
        "class UPSAMPLEBLOCK(nn.Module):\n",
        "  def __init__(self, in_channels, scale_factor):\n",
        "    super(UPSAMPLEBLOCK, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, \n",
        "                          in_channels * scale_factor **2,\n",
        "                          kernel_size = 3,\n",
        "                          stride = 1,\n",
        "                          padding = 1)\n",
        "    self.pxs = nn.PixelShuffle(scale_factor)\n",
        "    self.act = nn.PReLU(num_parameters = in_channels)\n",
        "  \n",
        "  def forward(self,input_tensor):\n",
        "    return self.act(self.pxs(self.conv(input_tensor)))\n",
        "\n",
        "\n",
        "class RESIDUALBLOCK(nn.Module):\n",
        "  ''' Residual block will be used only for building the generator\n",
        "  network\n",
        "  '''\n",
        "  def __init__(self, in_channels):\n",
        "    super(RESIDUALBLOCK, self).__init__()\n",
        "    self.conv1 = CNNBLOCK(in_channels, in_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv2 = CNNBLOCK(in_channels, in_channels, kernel_size = 3, stride = 1, padding = 1, use_act = False)\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    out = input_tensor\n",
        "    x = self.conv1(input_tensor)\n",
        "    x = self.conv2(x)\n",
        "    return x + out\n",
        "\n",
        "class GENERATOR(nn.Module):\n",
        "  ''' This is the ResNet with 16 residual blocks'''\n",
        "  def __init__(self, in_channels = 3, num_channels = 64, num_blocks = 16):\n",
        "    super(GENERATOR, self).__init__()\n",
        "    self.initial_block = CNNBLOCK(in_channels, \n",
        "                                  num_channels, \n",
        "                                  kernel_size = 9, \n",
        "                                  stride = 1,\n",
        "                                  padding = 4, \n",
        "                                  use_bn = False)\n",
        "    self.residuals = nn.Sequential(\n",
        "        *[RESIDUALBLOCK(num_channels) for _ in range(num_blocks)] # this create all 16 resblocks\n",
        "    )\n",
        "    self.convblock = CNNBLOCK(num_channels,\n",
        "                              num_channels, \n",
        "                              kernel_size = 3, \n",
        "                              stride = 1, \n",
        "                              padding = 1, \n",
        "                              use_act = False)\n",
        "    self.upsamples = nn.Sequential(UPSAMPLEBLOCK(in_channels = num_channels, scale_factor = 2),\n",
        "                                   UPSAMPLEBLOCK(in_channels = num_channels, scale_factor = 2))\n",
        "    self.finalconv = nn.Conv2d(in_channels = num_channels,\n",
        "                               out_channels = in_channels, \n",
        "                               kernel_size = 9,\n",
        "                               stride = 1,\n",
        "                               padding = 4)\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    initial = self.initial_block(input_tensor)\n",
        "    x = self.residuals(initial)\n",
        "    x = self.convblock(x) + initial\n",
        "    x = self.upsamples(x)\n",
        "    x = self.finalconv(x)\n",
        "    return torch.tanh(x)\n",
        "\n",
        "\n",
        "class DISCRIMINATOR(nn.Module):\n",
        "  ''' has similar architecture as vggnet'''\n",
        "  def __init__(self, in_channels = 3, \n",
        "               features = [64, 64, 128, 128, 256,256, 512,512]):\n",
        "    super(DISCRIMINATOR, self).__init__()\n",
        "    blocks = []\n",
        "    for idx, feature in enumerate(features):\n",
        "      blocks.append(\n",
        "          CNNBLOCK(in_channels,\n",
        "                   feature,\n",
        "                   kernel_size = 3,\n",
        "                   stride = 1 + idx % 2,\n",
        "                   padding = 1,\n",
        "                   discriminator = True,\n",
        "                   use_act = True,\n",
        "                   use_bn = False if idx == 0 else True))\n",
        "      in_channels = feature\n",
        "    self.blocks = nn.Sequential(*blocks)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((6,6)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = 512*6*6, out_features = 1024),\n",
        "        nn.LeakyReLU(0.2, inplace = True),\n",
        "        nn.Linear(in_features = 1024, out_features = 1))\n",
        "  \n",
        "  def forward(self, input_tensor):\n",
        "    x = self.blocks(input_tensor)\n",
        "    return self.classifier(x)\n",
        "  \n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjCXmtkwj5hH"
      },
      "source": [
        "# testing the network if it gives the desired outputs shapes:\n",
        "def __test__():\n",
        "  lR = 24 # we need to upscale to 3, 96, 96\n",
        "  with torch.cuda.amp.autocast():\n",
        "    lr_imgs = torch.randn(size = (32, 3, lR, lR))\n",
        "    generator = GENERATOR()\n",
        "    discriminator = DISCRIMINATOR()\n",
        "    gen_out = generator(lr_imgs)\n",
        "    disc_out = discriminator(gen_out)\n",
        "    print(f\">>>> The generator's shape: {gen_out.shape}\\n>>>> discriminator's shape: {disc_out.shape}\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPFu37fhpFTr",
        "outputId": "9c71db5a-9340-4cfc-abe0-1871db1b1293"
      },
      "source": [
        "__test__()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> The generator's shape: torch.Size([32, 3, 96, 96])\n",
            ">>>> discriminator's shape: torch.Size([32, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO3N8eRSTt7F"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVmW71RLpN29"
      },
      "source": [
        "# This model is trained using combination of loss components. \n",
        "# the perceptual loss is computed using vgg19-features and is \n",
        "# included  with GANS loss to optimize the network.\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "5c2999e50aed4816a12e5379c65e844a",
            "fae13debed2f49028e4ba8b5a6b67c88",
            "805ba895e43544128ae5535fe54b1170",
            "f28083dbdba14366948b988e45fdd8c8",
            "29429b6576454373a71d1f51787794d1",
            "cd92707c124d46e18e376d94411d4026",
            "5a7134d25897472ca7986e7e801628e8",
            "f3ac8bd9487b4cc988cf8ed1e8f1da49"
          ]
        },
        "id": "dbCLYInswgdW",
        "outputId": "dcc842ea-b87e-4b40-e8af-324ac3dbf154"
      },
      "source": [
        "# for vgg19 pretrained network layers considered are 0:36  i.e\n",
        "vgg = torchvision.models.vgg19(pretrained = True).features"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c2999e50aed4816a12e5379c65e844a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjvU1PPCw-pB",
        "outputId": "6401f4b9-3b57-4090-b8ce-0781cb6bc909"
      },
      "source": [
        "print(vgg)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (17): ReLU(inplace=True)\n",
            "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (24): ReLU(inplace=True)\n",
            "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (26): ReLU(inplace=True)\n",
            "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (31): ReLU(inplace=True)\n",
            "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (33): ReLU(inplace=True)\n",
            "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (35): ReLU(inplace=True)\n",
            "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulcsmb0FwF72"
      },
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "  ''' this class will compute the percetual loss using pretrained vgg19'''\n",
        "  def __init__(self):\n",
        "    super(PerceptualLoss, self).__init__()\n",
        "    self.vgg19 = torchvision.models.vgg19(pretrained = True).features[:36].eval().to(device = device)\n",
        "    self.loss = nn.MSELoss()\n",
        "     # we need not to train the vggnet again\n",
        "    for pars in self.vgg19.parameters():\n",
        "      pars.requires_grad = False\n",
        "  \n",
        "  def forward(self, LR, HR):\n",
        "    ''' \n",
        "    inputs is low resolution image (which is upgraded with the generator)\n",
        "    and output is the high resolution image (original images without distortion)\n",
        "    '''\n",
        "    vgg_LR_inputs = self.vgg19(LR)\n",
        "    vgg_HR_target = self.vgg19(HR)\n",
        "    # now we get the perceptual loss using vggnet as \n",
        "    return self.loss(vgg_LR_inputs, vgg_HR_target)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8KgGq8_a9YY"
      },
      "source": [
        "# Tensorboard directories:\n",
        "current_time = datetime.datetime.now().timestamp()\n",
        "fake_path = \"logs/tensorboard/generator_srgan/\"+str(current_time)\n",
        "real_path = \"logs/tensorboard/discriminator_srgan/\"+str(current_time)\n",
        "fake_writer = summary.create_file_writer(fake_path)\n",
        "real_writer = summary.create_file_writer(real_path)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGTiwSLgUX0b",
        "outputId": "4a40d26c-48a9-4833-9d3a-b64a127f6865"
      },
      "source": [
        "# testing the perceptual loss function if its gives what we needed:\n",
        "lR = torch.randn(size = (32, 3, 24,24)).to(device = device)\n",
        "hR = torch.randn(size = (32, 3, 96,96)).to(device = device)\n",
        "perceptual_loss = PerceptualLoss()\n",
        "print(f\">>>> The perceptual loss: {perceptual_loss.forward(lR, hR)}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> The perceptual loss: 0.26543715596199036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 512, 6, 6])) that is different to the input size (torch.Size([32, 512, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyOZDvbrXxQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d20e4c4-c06f-414e-a203-0a559042e006"
      },
      "source": [
        "# Hyperparameters to be used for training and preprocessing images\n",
        "learning_rate = 1e-4\n",
        "EPOCHS = 10\n",
        "batch_size = 16\n",
        "HR = 96\n",
        "LR = HR//4\n",
        "img_channels = 3\n",
        "\n",
        "transforms_hR = transforms.Compose(\n",
        "    [\n",
        "     transforms.Normalize(np.array([0.5 for _ in range(img_channels)]),np.array([0.5 for _ in range(img_channels)]))\n",
        "    \n",
        "    ]\n",
        ")\n",
        "\n",
        "transforms_lR = transforms.Compose(\n",
        "    [transforms.Resize(size = (LR, LR), interpolation = Image.BICUBIC),\n",
        "    transforms.Normalize(np.array([0 for _ in range(img_channels)]),np.array([1 for _ in range(img_channels)]))\n",
        "])\n",
        "\n",
        "transform_General = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.RandomCrop((HR, HR))                                       \n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "     transforms.Normalize(mean = [0,0,0], std = [1,1,1]),\n",
        "     transforms.ToTensor()\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wQR5uLw1Li8"
      },
      "source": [
        "os.chdir(\"/content/drive/MyDrive/flickr30k_images/flickr30k_images\")\n",
        "# ploting examples after training:\n",
        "def plot_examples(low_res_folder, gen):\n",
        "    files = os.listdir(low_res_folder)\n",
        "\n",
        "    gen.eval()\n",
        "    for file in files:\n",
        "        image = Image.open(\"/test1_SRGAN\" + file)\n",
        "        with torch.no_grad():\n",
        "          upscaled_img = gen(test_transform(np.asarray(image))\n",
        "                .unsqueeze(0)\n",
        "                .to(device = device)\n",
        "            )\n",
        "        save_image(upscaled_img * 0.5 + 0.5, f\"saved/{file}\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XqiVPeRfuVH"
      },
      "source": [
        "highres_transform = B.Compose(\n",
        "    [\n",
        "        B.Normalize(mean=np.array([0.5, 0.5, 0.5]), std=np.array([0.5, 0.5, 0.5])),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "lowres_transform = B.Compose(\n",
        "    [\n",
        "        B.Resize(width=LR, height=LR, interpolation=Image.BICUBIC),\n",
        "        B.Normalize(mean=np.array([0, 0, 0]), std=np.array([1, 1, 1])),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "both_transforms = B.Compose(\n",
        "    [\n",
        "        B.RandomCrop(width=HR, height= HR),\n",
        "        B.HorizontalFlip(p=0.5),\n",
        "        B.RandomRotate90(p=0.5),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pgyNzB1VZXd",
        "outputId": "b9dd05ba-4347-4930-e567-b044158fb5b6"
      },
      "source": [
        "# We use the flickr30k dataset for demo:\n",
        "class ImageFolder(Dataset):\n",
        "  def __init__(self, root_dir, csv_file, tr1 = None, tr2 = None, tr3 = None):\n",
        "    super(ImageFolder, self).__init__()\n",
        "    self.root_dir = root_dir\n",
        "    self.data = pd.read_csv(csv_file, error_bad_lines = False)\n",
        "    self.tr = True\n",
        "    self.tr1 = tr1\n",
        "    self.tr2 = tr2\n",
        "    self.tr3 = tr3\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    img_path = os.path.join(self.root_dir, self.data.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "    if self.tr:\n",
        "      image = self.tr1(image)\n",
        "      lr_image = self.tr2((image))\n",
        "      hr_image = self.tr3(image)\n",
        "    return lr_image, hr_image\n",
        "\n",
        "dataset = ImageFolder(root_dir = \"/content/drive/MyDrive/flickr30k_images/flickr8k/images\",\n",
        "                      csv_file = \"/content/drive/MyDrive/flickr30k_images/flickr8k/captions.txt\",\n",
        "                      tr1 = transform_General,\n",
        "                      tr2 = transforms_lR,\n",
        "                      tr3 = transforms_hR)\n",
        "\n",
        "loader = DataLoader(dataset = dataset, shuffle = True, batch_size = batch_size)\n",
        "LR_image, HR_image = next(iter(loader))\n",
        "print(f\">>>> LR_image shape: {LR_image.shape}\\n>>>> HR_image shape = {HR_image.shape}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>> LR_image shape: torch.Size([16, 3, 24, 24])\n",
            ">>>> HR_image shape = torch.Size([16, 3, 96, 96])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k2oSoZTMZt5"
      },
      "source": [
        "# Instantiating the model class and save to device\n",
        "generator = GENERATOR().to(device = device)\n",
        "discriminator = DISCRIMINATOR().to(device = device)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbu53Y0XXwKk"
      },
      "source": [
        "# Get the losses and optimizers objects:\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "vgg_loss = PerceptualLoss()\n",
        "mse = nn.MSELoss()\n",
        "gen_optimimizer = optim.Adam(params = generator.parameters(), lr = learning_rate, betas = (0.9, 0.999))\n",
        "disc_optimizer = optim.Adam(params = discriminator.parameters(), lr = learning_rate, betas = (0.9, 0.999))\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBXhscWOGxeB",
        "outputId": "d1f3fa62-6bb5-4f38-faa8-0c630936f1f3"
      },
      "source": [
        "# The training loop:\n",
        "step = 0\n",
        "tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"\\n>>>> training starts for epoch <{epoch + 1}>\\n>>>> please wait while the model is training.................................\")\n",
        "\n",
        "  for idx, (lR, hR) in enumerate(tqdm(loader)):\n",
        "    lR = lR.to(device = device)\n",
        "    hR = hR.to(device = device)\n",
        "    # training the discriminator on both hR and lR images\n",
        "    # we still have to maximize log(D(x) + log(1 - D(G(x))))\n",
        "\n",
        "    lR_fake = generator(lR) \n",
        "    lR_disc_preds = discriminator(lR_fake.detach()) # to re-use in the generator we detach the gradients\n",
        "    hR_disc_preds = discriminator(hR)\n",
        "    disc_real_loss = bce(hR_disc_preds, torch.ones_like(hR_disc_preds) - torch.rand_like(hR_disc_preds)*0.1)\n",
        "    disc_fake_loss = bce(lR_disc_preds, torch.zeros_like(lR_disc_preds))\n",
        "    disc_loss = (disc_real_loss + disc_fake_loss) / 2\n",
        "    discriminator.zero_grad()\n",
        "    disc_loss.backward()\n",
        "    disc_optimizer.step()\n",
        "\n",
        "    # training the generator: \n",
        "    # Here is when we utilize the vgg-loss but the main idea remain the same. [Max(log(D(G(x))))]\n",
        "    gen_out = discriminator(lR_fake)\n",
        "    #mse_loss = mse(lR, hR) # main purpose of SRGAN is to replace this component with the vgg-loss\n",
        "    adversarial_loss = 1e-3 * bce(gen_out, torch.ones_like(gen_out))\n",
        "    gen_vgg_loss = 0.006* vgg_loss(lR_fake, hR) # why 0.006????Not clear for me\n",
        "    gen_loss = adversarial_loss + gen_vgg_loss # leave out the mse_loss or take mse loss with the vgg-loss\n",
        "    generator.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    gen_optimimizer.step()\n",
        "\n",
        "    if idx% 200 == 0:\n",
        "      print(f\"\\n>>> end of epoch <{epoch + 1}>: generator loss:=>=>=>{gen_loss:.4f}: discriminator loss=>=>=>{disc_loss:.4f}\")\n",
        "      #plot_examples(\"/content/drive/MyDrive/flickr30k_images/flickr30k_images/\", generator)\n",
        "      with fake_writer.as_default():\n",
        "        summary.scalar(\"generator_loss\", gen_loss.cpu().detach().numpy(), step = step)\n",
        "      with real_writer.as_default():\n",
        "        summary.scalar('discriminator_loss', disc_loss.cpu().detach().numpy(), step = step)\n",
        "      step += 1\n",
        "%tensorboard --logdir logs/tensorboard\n",
        "toc = time.time()\n",
        "print(f\"\\n>>>> time elapsed for training srgan for 10 epochs: {time_fmt(toc - tic)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2529 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>>> training starts for epoch <1>\n",
            ">>>> please wait while the model is training.................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/2529 [00:04<3:20:49,  4.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>> end of epoch <1>: generator loss:=>=>=>0.0070: discriminator loss=>=>=>0.6795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 201/2529 [12:41<2:18:57,  3.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            ">>> end of epoch <1>: generator loss:=>=>=>0.0050: discriminator loss=>=>=>0.4164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 316/2529 [18:25<1:50:29,  3.00s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHezULzh4GUG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}